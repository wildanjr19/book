{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed9b806",
   "metadata": {},
   "source": [
    "## Pretraining On Unlabeled Data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2ab91",
   "metadata": {},
   "source": [
    "BAB 5  \n",
    "1. Menghitung training dan validation loss untuk menilai kualitas LLM-generated text saat pelatihan  \n",
    "2. Implementasi training-function dan pretraining LLM  \n",
    "3. Menyimpan dan load model weights untuk melanjutkan pelatihan LLM  \n",
    "4. Load pretraining weights dari OpenAi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a72e70c",
   "metadata": {},
   "source": [
    "### 5.1 Evaluating generative text models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6e06a",
   "metadata": {},
   "source": [
    "#### 5.1.1 Using GPT to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be946c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from GPTModel import GPTModel, GPT_CONFIG_124M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fee243",
   "metadata": {},
   "source": [
    "*Lihat `GPTModel.py`  \n",
    "Context length dirubah menjadi 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a042830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1705457",
   "metadata": {},
   "source": [
    "1. Menggunakan tokenizer untuk encode input teks menjadi representasi token ID  \n",
    "2. Ketika diberikan token ID, model akan memproduksi n logits vectors (baris), dimana setiap vektor memiliki 50,257 elemen (kolom) == vocabulay_size  \n",
    "3. Setelah convert logits menjadi token ID, kita menggunakan tokenizer untuk decode token ID kembali representasi teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba2064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from GenerateText import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10def73",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e8ad0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text : \n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "\n",
    "print(\"Output text : \\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572a0de",
   "metadata": {},
   "source": [
    "#### 5.1.2 Calculating the text generation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b10d42",
   "metadata": {},
   "source": [
    "Secara graph proses prediksi token atau kata berikutnya seperti ini :  \n",
    "1. Setiap kata dirubah menjadi token id berdasar indeks didalam vocabulary  \n",
    "2. Token id ini akan diproses oleh model sehingga menghasilkan vector berdimensi tinggi yang berbentuk probabilitas (setelah fungsi softmax, ketika diijumlahkan akan menjadi satu)  \n",
    "3. Kemudian mencari `argmax` dari setiap baris vektor, dan melihat indeks posisinya  \n",
    "4. Indeks dengan nilai tertinggi inilah yang menjadi tebakan atau prediksi kata selanjutnya  \n",
    "4. Kemudian (indeks )token ids dengan nilai tertinggi ini di konversi kembali menjadi kata sesuai dengan indeks dalam vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd7eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],     # ' every effort moves\n",
    "                       [40, 1107, 588]])        # 'i really like'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe1e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([[3626, 6100, 345],      # 'effort moves you'\n",
    "                        [1107, 588, 11311]])    # \"really like chocholate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6075fb73",
   "metadata": {},
   "source": [
    "Kemudian kita memasukann inputs diatas ke model untuk mengkalkulasi logist vectors, setelahnya melakukan fungsi softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c226143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # disable gradient tracking\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742da2a2",
   "metadata": {},
   "source": [
    "2 >> dua inputs  \n",
    "3 >> banyak token atau kata  \n",
    "50257 >> embedding dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2039cce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs : \n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs : \\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3709140e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1 :  effort moves you\n",
      "Outputs batch 1 :  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1 : {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1 : {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3874fc9",
   "metadata": {},
   "source": [
    "Pada saat melatih model, tujuannya adalah untuk **memaksimisasi/memaksimalkan** nilai pada indeks yang sesuai dengan **target**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc7af419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1 :  tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2 :  tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "\n",
    "print(\"Text 1 : \", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "\n",
    "print(\"Text 2 : \", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfca4f3",
   "metadata": {},
   "source": [
    "Bagaimana kita memaksimalkan softmax probability yang merupakan target token?  \n",
    "Caranya adalah memperbarui model weights. Sehingga, output dari model adalah nilai tertinggi yang merupakan token ids yang ingin kita generate (sesuai dengan target sebenarnya). Proses ini disebut **backpropagation**  \n",
    "**Backpropagation** sendiri membutuhkan *loss function*, yaitu menghitung selisih antara prediksi dari model dengan nilai sebenarnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "598763d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe507900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94a94ac",
   "metadata": {},
   "source": [
    "Goals kita adalah mendapatkan nilai rata-rat log probabilitas mendekati 0, dengan cara update model weights yang merupakan bagian dari pelatihan. Pada konteks deep learning praktik umum yang sering dilakukan bukanlah mendorong rata-rata log probabilites menuju 0, tapi menurunkan negative avg log probability mendekati 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bde837a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba209c",
   "metadata": {},
   "source": [
    "Pada konteks deep learning, mengubah negative values ini disebut dengan *cross entropy loss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d1db437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape :  torch.Size([2, 3, 50257])\n",
      "Targets shape :  torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# before we appluy the cross_entrpy funtion, lets briefly recall the shape of the logist\n",
    "print('logits shape : ', logits.shape)\n",
    "print('Targets shape : ', targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d165a",
   "metadata": {},
   "source": [
    "Dapat dilihat. Logits tensor memiliki tiga dimensi (batch_size, number of tokens, dan vocabulary size). Sedangkan target memiliki dua dimensi (batch_size, number of tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "373ab39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_flat shape :  torch.Size([6, 50257])\n",
      "targets_flat shape :  torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# before apply cross_entropy, we flatten the logits and targets tensors\n",
    "logits_flat = logits.flatten(0, 1) # (batch_size * context_length, vocab_size)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print('logits_flat shape : ', logits_flat.shape)\n",
    "print('targets_flat shape : ', targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e1246",
   "metadata": {},
   "source": [
    "PyTorch cross_entropy akan menghandle step-step yang telah kita lakukan sebelumnya (softmax + calculate log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c23a50a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d2e915",
   "metadata": {},
   "source": [
    "Hasilnya sama dengan apa yang telah kita lakuakan sebelumnya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499103d1",
   "metadata": {},
   "source": [
    "#### 5.1.3 Calculating the training and validation set loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fe9ebf",
   "metadata": {},
   "source": [
    "Untuk menghitung loss pada saat training dan validasi kita akan menggunakan dataset 'The Verdict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "167e5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads data\n",
    "file_path = 'the-verdict.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fba69dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters :  20479\n",
      "Total tokens :  5145\n"
     ]
    }
   ],
   "source": [
    "# check the numer of characters in dataset\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print('Total characters : ', total_characters)\n",
    "print('Total tokens : ', total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6839c9a",
   "metadata": {},
   "source": [
    "Hany dengan 5145 tokens, teks akan terasa ringan karena kita hanya inginn melatih model dalam hitungan menit saja alih-alih berminggu-minggu seperti model-model LLM besar lainnya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ff4bf",
   "metadata": {},
   "source": [
    "Selanjutnya kita akan membagi dataset menjadi training dan validation menggunakan data loader yang sudah dipelajari di chapter 2. Karena kendala spasial, kita akan menggunakan `max_length = 6`. Bagaimanapun, secara aktual pada data loaders, kita menggunakan `max_length = 256` agar LLM bisa melihat konteks yang lebih panjang pada saat training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79d5fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation \n",
    "train_ratio = 0.90\n",
    "split_dx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_dx]\n",
    "val_data = text_data[split_dx:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca348c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLoaderV1 import create_data_loader_v1\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_data_loader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M['context_length'],\n",
    "    stride=GPT_CONFIG_124M['context_length'],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_data_loader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M['context_length'],\n",
    "    stride=GPT_CONFIG_124M['context_length'],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4121dd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader : \n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader : \n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "print('Train loader : ')\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "\n",
    "print('\\nValidation loader : ')\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e7eaf",
   "metadata": {},
   "source": [
    "Dari output terlihat bahwa kita memiliki 9 set data training, dan satu set data untuk validasi. Setiap set terdiri dari 2 batch (teks), dan 256 token (kata). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b4214",
   "metadata": {},
   "source": [
    "Selanjutnya kita membuat fungsi untuk menghitung cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea3183ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    # memindahkan input ke GPU\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1),\n",
    "                                             target_batch.flatten())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fce2998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute the training and validation loss\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "        # menyampatkan jumlah batch dengan data loader\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, \n",
    "                target_batch,\n",
    "                model,\n",
    "                device\n",
    "            )\n",
    "            total_loss += loss.item() # sums of loss for each batch\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b2dcfb",
   "metadata": {},
   "source": [
    "Secara default, `calc_loss_loader` melakukan iterasi di semua batches pada data loader yang diberikan. Mengakumulasi nilai loss di `total_loss`, dan menghitung rata-ratanya. Alternatif, kita bisa menspesifikasi angka bathces yang lebih kecil via `num_bathces` untuk mempercepat proses evaluasi selama training model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bb1a7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss :  10.987583372328016\n",
      "Validation loss : 10.981104850769043\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device) # set to the GPU\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print('Train loss : ', train_loss)\n",
    "print('Validation loss :', val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478af5ae",
   "metadata": {},
   "source": [
    "### 5.2 Training an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab24a6e",
   "metadata": {},
   "source": [
    "Tips untuk melatih model LLM adalah menggunakan loop, yang berguna agar code tetap konsisten dan mudah dibaca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06c8d5",
   "metadata": {},
   "source": [
    "Secara umum ada 8 langkah. Dimulai dari iterasi dari setiap epoch, memproses batch, mereset gradient, menghitung loss dan gradient baru, update bobot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbb18a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq\n",
    "                       ,eval_iter, start_context, tokenizer):\n",
    "    # initialize list to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # start the main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # reset loss gradients from the previous batch iteration\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            # calculate loss gradient\n",
    "            loss.backward()\n",
    "            # update model weights using loss gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}) : \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\"\n",
    "                      )\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4a95e",
   "metadata": {},
   "source": [
    "- token seen : jumlah total token unik atau token yang telah dipaparkan (dilihat) oleh model LLM selama proses pelatihannya  \n",
    "- global step : singkatnya, global step adalah hitungan kumulatif dari batch yang telah diproses, sementara epoch adalah hitungan berapa kali seluruh dataset telah dilewati. Global step memberikan pandangan yang lebih detail tentang kemajuan pelatihan secara keseluruhan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1975f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval() # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train() # set model back to training mode\n",
    "\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec12d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval() # set model to evaluation mode\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = generate_text_simple(\n",
    "            model=model,\n",
    "            idx=encoded,\n",
    "            max_new_tokens=50,\n",
    "            context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(output, tokenizer)\n",
    "\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train() # set model back to training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaa8a9b",
   "metadata": {},
   "source": [
    "Hampir sama dengan `evaluate_model`, fungsi `generate_and_print_sample` digunakan untuk tracking apakah performa model meningkat selama proses pelatihan. Fungsi `generate_and_print_sample` menerima `start_context` sebagai input, mengubahnya menjadi token ids, dan memasukkannya kedalam model LLM untuk membuat sample text menggunakan fungsi `generate_and_print_sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "761523d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), # returns all trainable partameters of the model \n",
    "    lr = 0.0004,\n",
    "    weight_decay=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f787097",
   "metadata": {},
   "source": [
    "AdamW adalah varian dari adam yang meningkatkan weight_decay yang bertujuan untuk kompleksitas dari model dan menghindari overfitting dengan memberikan penalti untuk bobot yang besar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cf666c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000) : Train loss 9.821, Val loss 9.931\n",
      "Ep 1 (Step 000005) : Train loss 8.069, Val loss 8.334\n",
      "Every effort moves you,,,,,,,,,,,,,,.                                   \n",
      "Ep 2 (Step 000010) : Train loss 6.623, Val loss 7.049\n",
      "Ep 2 (Step 000015) : Train loss 6.046, Val loss 6.598\n",
      "Every effort moves you, and,, and,,,,,,,,,.                                   \n",
      "Ep 3 (Step 000020) : Train loss 5.558, Val loss 6.503\n",
      "Ep 3 (Step 000025) : Train loss 5.471, Val loss 6.392\n",
      "Every effort moves you, and to the to the of the to the, and I had. Gis, and I had, and, and, and, and I had, and, and, and, and, and, and, and, and, and,\n",
      "Ep 4 (Step 000030) : Train loss 4.995, Val loss 6.275\n",
      "Ep 4 (Step 000035) : Train loss 4.756, Val loss 6.289\n",
      "Every effort moves you, and I had been the picture.                    \"I\"I the the donkey of the donkey the donkey of the picture and I had been a\"I\n",
      "Ep 5 (Step 000040) : Train loss 4.113, Val loss 6.182\n",
      "Every effort moves you know the \"Oh, and he had to me--I me. \"Oh, I felt--and it's the  \"Oh, and I had been the donkey--and it to me, and down the \"Oh,\n",
      "Ep 6 (Step 000045) : Train loss 3.721, Val loss 6.143\n",
      "Ep 6 (Step 000050) : Train loss 3.170, Val loss 6.147\n",
      "Every effort moves you know the fact, and I felt.  \"I had the last word.     \"I didn't. \"I was his pictures--I looked.   \"I looked.    \"I\n",
      "Ep 7 (Step 000055) : Train loss 3.109, Val loss 6.186\n",
      "Ep 7 (Step 000060) : Train loss 2.370, Val loss 6.126\n",
      "Every effort moves you know the inevitable garlanded to have to have the fact with a little: \"Yes--and by me to me to have to see a smile behind his pictures--as I had been the honour of the donkey.      \n",
      "Ep 8 (Step 000065) : Train loss 1.911, Val loss 6.151\n",
      "Ep 8 (Step 000070) : Train loss 1.592, Val loss 6.223\n",
      "Every effort moves you?\"  \"Yes--I glanced after him, and uncertain. \"Oh, he was's an awful simpleton, and Mrs. Gisburn's head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075) : Train loss 1.239, Val loss 6.259\n",
      "Ep 9 (Step 000080) : Train loss 0.957, Val loss 6.270\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. The last word.        He laughed again, and threw back the head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085) : Train loss 0.708, Val loss 6.387\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c29fd9e",
   "metadata": {},
   "source": [
    "Agar mudah melihat performa dari model pada saat pelatihan, mari lakukan visualisasi plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54dc920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    ax1.plot(epochs_seen, train_losses, label='Training Loss')\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle='-.', label='Validatio Loss')\n",
    "\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel('Token seen')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "616ab541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epoch_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ecc43",
   "metadata": {},
   "source": [
    "Training plot pelatihan dan validasi yang dihasilkan, seperti yang dapat kita lihat, kerugian pelatihan dan validasi mulai membaik pada epoch pertama. Namun, kerugian mulai menyimpang setelah epoch kedua. Divergensi ini dan fakta bahwa kerugian validasi jauh lebih besar daripada kerugian pelatihan menunjukkan bahwa model tersebut overfitting dengan data pelatihan. Kita dapat memastikan bahwa model tersebut mengingat data pelatihan kata demi kata dengan mencari potongan teks yang dihasilkan, seperti ironi yang tidak masuk akal dalam berkas teks \"The Verdict\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99918a54",
   "metadata": {},
   "source": [
    "Overfitting ini terinndikasi karena kita melatih model dengan dataset yang kecil dan melatihnya dengan beberapa epoch. Pada umumnya model dilatih pada dataset yang cukup masif untuk satu epoch saja. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84642d5",
   "metadata": {},
   "source": [
    "### 5.3 Decoding strategies to control randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693758e5",
   "metadata": {},
   "source": [
    "Generation strategies atau disebut decoding strategies yang digunakan untuk menggenerasi teks-teks original. Ada dua teknik untuk meningkatkan dalam generasi teks yaitu **temperature scaling** dan **top-k sampling**.  \n",
    "Kita mulai dengan merubah device kembali ke CPU, karen inferensi tidak terlalu membutuhkan sumber daya yang tinggi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f9ab705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c22ac",
   "metadata": {},
   "source": [
    "Selanjutnya kita memasangkan `GPTModel` pada `generate_text_sample` untuk menggenerasi token dalam satu waktu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9661e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text : \n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids('Every effort moves you', tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "\n",
    "print(\"Output text : \\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa70e6d",
   "metadata": {},
   "source": [
    "Dari output yang diberikan teks yang dihasilkan pada proses generasi teks sesuai dengan teks yang memiliki probabilitas tertinggi diantara semua token di vocab. Ini artinya setiap kali kita run LLM akan menghasilkan hasil yang sama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a063f5",
   "metadata": {},
   "source": [
    "#### 5.3.1 Temperature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d92c1",
   "metadata": {},
   "source": [
    "Temperature scaling adalah teknik yang menambahkan pengaturan distribusi probabilitas pada next-token generation. Sebelumnya pada `generate_text_simple`, kita selalu melakukan pemilihan token setelahnya menggunakan probabilitas yang tertinggi via `torch.argmax` yang bisa disebut *greedy dicoding*. Untuk hasil generasi yang lebih bervariasi, kita dapat mengganti `argmax` dengan fungsi sampling probabilitas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "671fdcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    'closer' : 0,\n",
    "    'every' : 1,\n",
    "    'effort' : 2,\n",
    "    'forward' : 3,\n",
    "    'inches' : 4,\n",
    "    'moves' : 5,\n",
    "    'pizza' : 6,\n",
    "    'toward' : 7,\n",
    "    'you' : 8,\n",
    "}\n",
    "\n",
    "# invers vocab, sehingga menghasilkan pasangan key-value (dibalik)\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ed5712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume LLM has next-token logits for 'every effort moves you'\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a061fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "# convert logits to probabilities via softmax\n",
    "# obtain ID via argmax\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item() # mengembalikan index\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c519a942",
   "metadata": {},
   "source": [
    "Untuk mengimplementasikan proses sampling probabilistik, kita dapat menggantikan `argmax` dengan `multinomial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e6467d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2220ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item()\n",
    "              for _ in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba2cab8",
   "metadata": {},
   "source": [
    "Dapat dilihat, kata-kata lain muncul cukup banyak. Jika kita menerapkan ini pada proses generasi teks di LLM maka hasil next-token yang diberikan bisa variatif."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf183da",
   "metadata": {},
   "source": [
    "Lebih jauh kita bisa mengontrol pengaturan dari distribusi via **temperature scaling**. Ini adalah istilah lain dari membagi logits dengan angka yang lebih dari 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bbd0e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d803f0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures = [1, 0.1, 5]\n",
    "\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T)\n",
    "                 for T in temperatures]\n",
    "\n",
    "x = torch.arange(len(vocab))\n",
    "\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for i,  T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i],\n",
    "                   bar_width, label=f\"Temperature : {T}\")\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation = 90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d3f04",
   "metadata": {},
   "source": [
    "**Efek temperature**  \n",
    "- Temperature > 1  \n",
    "Membuat distribusi probabilitas lebih seragam (mendekati distribusi uniform). Model akan menghasilkan prediksi yang cenderung acak dan variatif  \n",
    "- Temperature < 1  \n",
    "Membuat distribusi probabilitas lebih tajam, sehingga token dengan probabilitas tertinggi lebih dominan. Model menghasilkan output yang lebih deterministik, dan paling mungkin sesuai konteks  \n",
    "- Temperature 1  \n",
    "Distribusi probabilitas tetap seperti aslinya  \n",
    "- Temperature 0  \n",
    "Secara efektif memilih token dengan probabilitas tertinggi (greedy decoding). Output sangat deterministik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b58b3b2",
   "metadata": {},
   "source": [
    "#### 5.3.2 Top-k sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dfc648",
   "metadata": {},
   "source": [
    "Pada top-k sampling kita dapat membatasi token yang disampling untuk menjadi top-k token dan tidak menghiraukan token yang tidak termasuk dengan melakukan masking pada probability scoresnya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67649d0",
   "metadata": {},
   "source": [
    "Untuk token-token yang tidak termasuk dalam top-k tokens dapat dimasking menggunakan negative infinity (-inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c802f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits :  tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions :  tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits : \", top_logits)\n",
    "print(\"Top positions : \", top_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fcd11",
   "metadata": {},
   "source": [
    "Dengan fungsi `where` pada PyTorch kita bisa menerapkan top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ea4f8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=\n",
    "    next_token_logits < top_logits[-1], # mengambil kondisi dimana nilai yang kurang dari top_k\n",
    "    input=torch.tensor(float('-inf')),\n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cab69da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# apply softmax funcrtion to new_logits\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722661a4",
   "metadata": {},
   "source": [
    "Sekarang kita dapat menerapkan temperature scaling dan multinomial function untuk sampling probabilitas untuk memilih token selanjutnya diantara 3 probabilitas tertinggi (top-k). Kita melakukan ini dengan memodifikasi fungsi generasi teks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89bc4d",
   "metadata": {},
   "source": [
    "#### 5.3.3 Modifying the text generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eafd6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # top_k sampling\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')),\n",
    "                logits\n",
    "            )\n",
    "        \n",
    "        # temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # greedy sampling\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        \n",
    "        # stop if end of seq token is reached\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f061d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text : \n",
      " Every effort moves you know began to my surprise, one of the face the irony. She wanted\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids('Every effort moves you', tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M['context_length'],\n",
    "    top_k=5,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print('Output text : \\n', token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f08339",
   "metadata": {},
   "source": [
    "### 5.4 Loading and saving model weights in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78965578",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b498f41",
   "metadata": {},
   "source": [
    "Kemudian kita dapat memuat bobot model ke GPTModel yang baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7c5c575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load('model.pth', map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7d945",
   "metadata": {},
   "source": [
    "Seperti yang didiskusikan di Chapter 4, dropout membantu model untuk menghindari overfitting pada pelatihan dengan cara mematikan beberapa layer saat pelatihan. Pada saat inferensi (prediksi) kita tidak ingin secara acak mematikan informasi yang telah dipelajari oleh model.  \n",
    "Dengan `model.eval()` akan mengubah model menjadi mode evaluasi, mematikan dropout layer. Jika kita ada rencana untuk melanjutkan pretraining di kemudian waktu, menyimpan state dari optimizer direkomendasikan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea74f25",
   "metadata": {},
   "source": [
    "Optimizer adaptive seperti AdamW menyimpan tambahan parameter untuk setiap bobot model. AdamW menggunakan histori dari data untuk menyesuaikan learning rate untuk setiap parameter model secara dinamis. Tanpa ini, optimizer akan ter-reset dan model akan sulit untuk konvergen dapat menyebabkan kegagalan.  \n",
    "Dengan `torch.save` kita dapat menyimpan model dan juga parameter `state_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1293c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\" : model.state_dict(),\n",
    "    \"optimizer_state_dict\" : optimizer.state_dict()},\n",
    "    \"model_and_optimizer.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9a89c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f53897",
   "metadata": {},
   "source": [
    "### 5.5 Loading pretrained weights from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f791d8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x2895b1dce50>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca8330",
   "metadata": {},
   "source": [
    "Selesai mendownload dan tersimpan di lokal kita, setelahnya kita harus melakukan pengecekan untuk memastikan ini disimpan secara benar dan mengandung kode python yang valid.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0dce63fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [10:31<00:00, 789kiB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size='124M',\n",
    "    models_dir='gpt2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "525fd5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings :  {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary key :  dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings : \", settings)\n",
    "print(\"Parameter dictionary key : \", params.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6fa8e0",
   "metadata": {},
   "source": [
    "- `settings` berisi tentang konfigurasi dari arsitektur LLM  \n",
    "- `params` berisi tentang bobot tensor aktual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "470f9004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimension : (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "# melihat beberapa bobot parameter\n",
    "print(params['wte'])\n",
    "print('Token embedding weight tensor dimension :', params['wte'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2316e",
   "metadata": {},
   "source": [
    "Setelah kita berhasil mme-load model dari GPT OpenAI kita harus mentransfer mereka dari `settinggs` dan `params` ke `GPTModel` yang telah kita buat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "186092fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list perbedaan arsitektur di setiap model\n",
    "model_configs = {\n",
    "    'gpt2-small (124M)' : {'emd_dim' : 768, 'n_layers' : 12, 'n_heads' : 12},\n",
    "    'gpt2-medium (355M)' : {'emd_dim' : 1024, 'n_layers' : 24, 'n_heads' : 16},\n",
    "    'gpt2-large (774M)' : {'emd_dim' : 1280, 'n_layers' : 36, 'n_heads' : 20},\n",
    "    'gpt2-xl (1558M)' : {'emd_dim' : 1600, 'n_layers' : 48, 'n_heads' : 25}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7dd569c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proses update atau transfer learning\n",
    "model_name = 'gpt2-small (124M)'\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7fa22",
   "metadata": {},
   "source": [
    "Model GPT kita dilatih dengan 256-token length, tapi versi asli dari GPT-2 OpenAI dilatih dengan 1024-token length. Jadi kita perlu melakukan update pada `context_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5b46ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({'context_length' : 1024})\n",
    "NEW_CONFIG[\"qkv_bias\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3efbf8",
   "metadata": {},
   "source": [
    "Sekarang kita dapat menggunakan model yang arsitekturnya telah kita update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1efbe237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f38253bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f'Shape mismatch. Left: {left.shape}, Right: {right.shape}')\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "200ae395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):          \n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte']) \n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):    \n",
    "        q_w, k_w, v_w = np.split(                           \n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1) \n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T) \n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T) \n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split( \n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign( \n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign( \n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign( \n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "        \n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"]) \n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"]) \n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb956b65",
   "metadata": {},
   "source": [
    "Pada fungsi `load_weights_into_gpt` kita secara hati-hati mencocokan bobot dari OpenAI dengan `GPTModel` kita. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ca5e567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformersBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load OpenAI into our model\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7cd46b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text : \n",
      " Every effort moves you as far as the hand can go until the end of your turn unless something happens\n",
      "\n",
      "This would remove you from a battle you lost due to an encounter on an opponent's power you controlled\n",
      "\n",
      "If you've defeated 10-20% of the\n"
     ]
    }
   ],
   "source": [
    "# use to generate text\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids('Every effort moves you', tokenizer).to(device),\n",
    "    max_new_tokens=50,\n",
    "    context_size=NEW_CONFIG['context_length'],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print('Output text : \\n', token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
